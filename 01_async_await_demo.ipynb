{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0a1c637",
   "metadata": {},
   "source": [
    "## Check python Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9419c331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###### Installed Libraries ############\n",
      "jiter==0.12.0\n",
      "ipykernel==7.1.0\n",
      "pexpect==4.9.0\n",
      "uvicorn==0.38.0\n",
      "types-requests==2.32.4.20250913\n",
      "requests==2.32.5\n",
      "traitlets==5.14.3\n",
      "ipython==9.8.0\n",
      "nest-asyncio==1.6.0\n",
      "click==8.3.1\n",
      "asttokens==3.0.1\n",
      "ptyprocess==0.7.0\n",
      "sse-starlette==3.0.3\n",
      "colorama==0.4.6\n",
      "jupyter_client==8.7.0\n",
      "jsonschema-specifications==2025.9.1\n",
      "executing==2.2.1\n",
      "pydantic==2.12.5\n",
      "tornado==6.5.3\n",
      "debugpy==1.8.18\n",
      "python-dotenv==1.2.1\n",
      "anyio==4.12.0\n",
      "griffe==1.15.0\n",
      "jedi==0.19.2\n",
      "idna==3.11\n",
      "cffi==2.0.0\n",
      "tqdm==4.67.1\n",
      "ipython_pygments_lexers==1.1.1\n",
      "appnope==0.1.4\n",
      "prompt_toolkit==3.0.52\n",
      "parso==0.8.5\n",
      "httpcore==1.0.9\n",
      "decorator==5.2.1\n",
      "certifi==2025.11.12\n",
      "stack-data==0.6.3\n",
      "charset-normalizer==3.4.4\n",
      "PyJWT==2.10.1\n",
      "jsonschema==4.25.1\n",
      "pydantic-settings==2.12.0\n",
      "matplotlib-inline==0.2.1\n",
      "h11==0.16.0\n",
      "sniffio==1.3.1\n",
      "python-dateutil==2.9.0.post0\n",
      "pydantic_core==2.41.5\n",
      "openai==2.11.0\n",
      "typing_extensions==4.15.0\n",
      "jupyter_core==5.9.1\n",
      "packaging==25.0\n",
      "mcp==1.24.0\n",
      "referencing==0.37.0\n",
      "python-multipart==0.0.20\n",
      "pure_eval==0.2.3\n",
      "distro==1.9.0\n",
      "Pygments==2.19.2\n",
      "attrs==25.4.0\n",
      "platformdirs==4.5.1\n",
      "httpx==0.28.1\n",
      "cryptography==46.0.3\n",
      "starlette==0.50.0\n",
      "urllib3==2.6.2\n",
      "openai-agents==0.6.3\n",
      "psutil==7.1.3\n",
      "typing-inspection==0.4.2\n",
      "annotated-types==0.7.0\n",
      "httpx-sse==0.4.3\n",
      "wcwidth==0.2.14\n",
      "rpds-py==0.30.0\n",
      "six==1.17.0\n",
      "pycparser==2.23\n",
      "pyzmq==27.1.0\n",
      "comm==0.2.3\n",
      "\n",
      "###### Python Environment Details ######\n",
      "Python Version       : 3.12.11\n",
      "Python Implementation: CPython\n",
      "Executable Path      : /Users/dev/gitdir/agentic_ai/.venv/bin/python\n",
      "Platform             : Darwin 25.1.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import sys\n",
    "import importlib.metadata\n",
    "\n",
    "##############################################\n",
    "print(f\"\\n###### Installed Libraries ############\")\n",
    "for dist in importlib.metadata.distributions():\n",
    "    print(f\"{dist.metadata['Name']}=={dist.version}\")\n",
    "\n",
    "# Print the Python version and other details\n",
    "print(f\"\\n###### Python Environment Details ######\")\n",
    "print(f\"Python Version       : {platform.python_version()}\")\n",
    "print(f\"Python Implementation: {platform.python_implementation()}\")\n",
    "print(f\"Executable Path      : {sys.executable}\")\n",
    "print(f\"Platform             : {platform.system()} {platform.release()}\\n\\n\")\n",
    "##############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35814cad",
   "metadata": {},
   "source": [
    "## Add simple Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5789f3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know. I can only answer questions about computer networking.\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, Runner\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "\n",
    "net_expert_instruction = \"\"\"\n",
    "You are a senior network engineer.\n",
    "\n",
    "Your domain of expertise is LIMITED to:\n",
    "- Computer networks (LAN, WAN, routing, switching, firewalls, load balancers)\n",
    "- Network protocols (TCP, UDP, IP, BGP, OSPF, HTTP, HTTPS, DNS, etc.)\n",
    "- Network design, troubleshooting, and performance\n",
    "\n",
    "HARD RULE:\n",
    "- If the user asks ANYTHING that is NOT clearly about computer networking or network protocols,\n",
    "  you MUST respond ONLY with exactly this sentence:\n",
    "  \"I don't know. I can only answer questions about computer networking.\"\n",
    "\n",
    "- Do NOT try to be helpful outside your domain.\n",
    "- Do NOT guess.\n",
    "- Do NOT explain topics from physics, math, biology, history, programming (unless directly about networking), or any other area.\n",
    "\"\"\"\n",
    "\n",
    "network_expert_agent = Agent(\n",
    "    name=\"Network Expert Agent\",\n",
    "    instructions=net_expert_instruction,\n",
    "    model=\"gpt-5-nano\"\n",
    "    )\n",
    "\n",
    "result = await Runner.run(network_expert_agent, \"What is Gravity?\")\n",
    "print(result.final_output)\n",
    "\n",
    "# result = Runner.run(network_expert_agent, \"What is VLAN?\")\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939be1b4",
   "metadata": {},
   "source": [
    "## Simple Python Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83d4c9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slept for 2 seconds\n",
      "Slept for 5 seconds\n"
     ]
    }
   ],
   "source": [
    "def sleep_func(seconds: int):\n",
    "    \"\"\"A simple function that sleeps for a given number of seconds.\"\"\"\n",
    "    import time\n",
    "    time.sleep(seconds)\n",
    "    return f\"Slept for {seconds} seconds\"\n",
    "\n",
    "print(sleep_func(2))\n",
    "print(sleep_func(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c95af6",
   "metadata": {},
   "source": [
    "## Simple Async Coroutine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b83a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:38:24 Sleeping for 4 seconds...\n",
      "16:38:24 Sleeping for 2 seconds...\n",
      "16:38:24 Sleeping for 3 seconds...\n",
      "16:38:26 Slept for 2 seconds\n",
      "16:38:27 Slept for 3 seconds\n",
      "16:38:28 Slept for 4 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['16:38:28 Slept for 4 seconds',\n",
       " '16:38:26 Slept for 2 seconds',\n",
       " '16:38:27 Slept for 3 seconds']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "import datetime\n",
    "# import nest_asyncio\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "def now():\n",
    "    return datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "async def sleep_async_func(seconds: int):\n",
    "    \"\"\"A simple function that sleeps for a given number of seconds.\"\"\"\n",
    "    print(f\"{now()} Sleeping for {seconds} seconds...\")\n",
    "    await asyncio.sleep(seconds)\n",
    "    print(f\"{now()} Slept for {seconds} seconds\")\n",
    "    return f\"{now()} Slept for {seconds} seconds\"\n",
    "\n",
    "# await sleep_async_func(2)\n",
    "\n",
    "await asyncio.gather(\n",
    "    sleep_async_func(4),\n",
    "    sleep_async_func(2),\n",
    "    sleep_async_func(3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cf3b50",
   "metadata": {},
   "source": [
    "## OpenAI Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "183fd4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OPENAI_API_KEY is set (length: 164 characters)\n",
      "ðŸ¤– Calling LLM with prompt: Tell me a joke about cat\n",
      "ðŸ“ Here's a cat joke:\n",
      "\n",
      "Why did the cat sit on the computer? It wanted to keep an eye on the mouse.\n",
      "\n",
      "Want a few more options?\n",
      "- What do you call a pile of kittens? A meowtain.\n",
      "- What do you call a cat that loves to swim? A catfish.\n",
      "- Why don't cats play poker in the jungle? Too many cheetahs.\n",
      "ðŸ¤– Calling LLM with prompt: What is the capital of France\n",
      "ðŸ“ The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Check if OpenAI API key is set\n",
    "api_key = os.environ.get('OPENAI_API_KEY')\n",
    "if api_key:\n",
    "    print(f\"âœ… OPENAI_API_KEY is set (length: {len(api_key)} characters)\")\n",
    "else:\n",
    "    print(\"âœ— OPENAI_API_KEY is not set\")\n",
    "    sys.exit(\"Error: OPENAI_API_KEY environment variable not set\")\n",
    "    \n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def llm_call(prompt: str):\n",
    "    print(\"ðŸ¤– Calling LLM with prompt:\", prompt)\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-5-nano\",\n",
    "        input=prompt\n",
    "    )\n",
    "    return f\"ðŸ“ {response.output_text}\"\n",
    "\n",
    "print(llm_call(\"Tell me a joke about cat\"))\n",
    "print(llm_call(\"What is the capital of France\"))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3317579c",
   "metadata": {},
   "source": [
    "## Implement Async in OpenAI Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3e0a2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:59:52 Calling LLM with prompt: Tell me a joke about cats.\n",
      "16:59:52 Calling LLM with prompt: Give me a fun fact about space.\n",
      "16:59:52 Calling LLM with prompt: What's the capital of France?\n",
      "16:59:52 Calling LLM with prompt: What is Gravity\n",
      "16:59:55 What's the capital of France? - Took: 2.9960854579985607 seconds\n",
      "*************************\n",
      "*  What's the capital of France? - Output: Paris\n",
      "*************************\n",
      "16:59:55 Tell me a joke about cats. - Took: 3.233744832999946 seconds\n",
      "*************************\n",
      "*  Tell me a joke about cats. - Output: Here's a cat joke for you: Why did the cat sit on the computer? It wanted to keep an eye on the mouse.\n",
      "\n",
      "Want another one?\n",
      "*************************\n"
     ]
    },
    {
     "ename": "TimeoutError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/tasks.py:520\u001b[39m, in \u001b[36mwait_for\u001b[39m\u001b[34m(fut, timeout)\u001b[39m\n\u001b[32m    519\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m timeouts.timeout(timeout):\n\u001b[32m--> \u001b[39m\u001b[32m520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fut\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mcall_llm_async\u001b[39m\u001b[34m(prompt)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnow()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Calling LLM with prompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.to_thread(client.responses.create,\n\u001b[32m     20\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mgpt-5-nano\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     21\u001b[39m     \u001b[38;5;28minput\u001b[39m=prompt\n\u001b[32m     22\u001b[39m )\n\u001b[32m     23\u001b[39m end_time = time.perf_counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/threads.py:25\u001b[39m, in \u001b[36mto_thread\u001b[39m\u001b[34m(func, *args, **kwargs)\u001b[39m\n\u001b[32m     24\u001b[39m func_call = functools.partial(ctx.run, func, *args, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.run_in_executor(\u001b[38;5;28;01mNone\u001b[39;00m, func_call)\n",
      "\u001b[31mCancelledError\u001b[39m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m: response.output_text, \u001b[33m\"\u001b[39m\u001b[33mtime_taken\u001b[39m\u001b[33m\"\u001b[39m: total_time} \n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# result = await asyncio.gather(\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m#     call_llm_async(\"Tell me a joke about cats.\"),\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m#     call_llm_async(\"Give me a fun fact about space.\"),\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m#     call_llm_async(\"What's the capital of France?\"),\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m#     call_llm_async(\"What is Gravity\")\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.wait_for(\n\u001b[32m     38\u001b[39m     asyncio.gather(\n\u001b[32m     39\u001b[39m     call_llm_async(\u001b[33m\"\u001b[39m\u001b[33mTell me a joke about cats.\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     40\u001b[39m     call_llm_async(\u001b[33m\"\u001b[39m\u001b[33mGive me a fun fact about space.\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     41\u001b[39m     call_llm_async(\u001b[33m\"\u001b[39m\u001b[33mWhat\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms the capital of France?\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     42\u001b[39m     call_llm_async(\u001b[33m\"\u001b[39m\u001b[33mWhat is Gravity\u001b[39m\u001b[33m\"\u001b[39m)),\n\u001b[32m     43\u001b[39m     timeout=\u001b[32m5\u001b[39m\n\u001b[32m     44\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/tasks.py:519\u001b[39m, in \u001b[36mwait_for\u001b[39m\u001b[34m(fut, timeout)\u001b[39m\n\u001b[32m    516\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.CancelledError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    517\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m timeouts.timeout(timeout):\n\u001b[32m    520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fut\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/timeouts.py:115\u001b[39m, in \u001b[36mTimeout.__aexit__\u001b[39m\u001b[34m(self, exc_type, exc_val, exc_tb)\u001b[39m\n\u001b[32m    110\u001b[39m     \u001b[38;5;28mself\u001b[39m._state = _State.EXPIRED\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._task.uncancel() <= \u001b[38;5;28mself\u001b[39m._cancelling \u001b[38;5;129;01mand\u001b[39;00m exc_type \u001b[38;5;129;01mis\u001b[39;00m exceptions.CancelledError:\n\u001b[32m    113\u001b[39m         \u001b[38;5;66;03m# Since there are no new cancel requests, we're\u001b[39;00m\n\u001b[32m    114\u001b[39m         \u001b[38;5;66;03m# handling this.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc_val\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01mis\u001b[39;00m _State.ENTERED:\n\u001b[32m    117\u001b[39m     \u001b[38;5;28mself\u001b[39m._state = _State.EXITED\n",
      "\u001b[31mTimeoutError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import time, datetime\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def now():\n",
    "    return datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "\n",
    "code_start = time.perf_counter()\n",
    "\n",
    "async def call_llm_async(prompt: str):\n",
    "    start_time = time.perf_counter()\n",
    "    print(f\"{now()} Calling LLM with prompt: {prompt}\")\n",
    "    \n",
    "    response = await asyncio.to_thread(client.responses.create,\n",
    "        model=\"gpt-5-nano\",\n",
    "        input=prompt\n",
    "    )\n",
    "    end_time = time.perf_counter()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"{now()} {prompt} - Took: {total_time} seconds\")\n",
    "    print(f\"{'*'*25}\\n*  {prompt} - Output: {response.output_text}\\n{'*'*25}\")\n",
    "    return {\"response\": response.output_text, \"time_taken\": total_time} \n",
    "\n",
    "\n",
    "# result = await asyncio.gather(\n",
    "#     call_llm_async(\"Tell me a joke about cats.\"),\n",
    "#     call_llm_async(\"Give me a fun fact about space.\"),\n",
    "#     call_llm_async(\"What's the capital of France?\"),\n",
    "#     call_llm_async(\"What is Gravity\")\n",
    "# )\n",
    "\n",
    "result = await asyncio.wait_for(\n",
    "    asyncio.gather(\n",
    "    call_llm_async(\"Tell me a joke about cats.\"),\n",
    "    call_llm_async(\"Give me a fun fact about space.\"),\n",
    "    call_llm_async(\"What's the capital of France?\"),\n",
    "    call_llm_async(\"What is Gravity\")),\n",
    "    timeout=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c611db2e",
   "metadata": {},
   "source": [
    "## Handle Timeout Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dec5cfb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:04:12 Calling LLM with prompt: What is Relativity theory\n",
      "17:04:12 Calling LLM with prompt: Tell me a joke about cats.\n",
      "17:04:12 Calling LLM with prompt: What's the capital of India?\n",
      "17:04:12 Calling LLM with prompt: Give me a fun fact about space.\n",
      "17:04:12 Calling LLM with prompt: What's the capital of France?\n",
      "17:04:12 Calling LLM with prompt: What is Gravity\n",
      "17:04:15 What's the capital of India? - Took: 3.2523026670023683 seconds\n",
      "*************************\n",
      "*  What's the capital of India? - Output: New Delhi.\n",
      "*************************\n",
      "17:04:15 What's the capital of France? - Took: 3.2553713749985036 seconds\n",
      "*************************\n",
      "*  What's the capital of France? - Output: Paris.\n",
      "*************************\n",
      "17:04:16 Tell me a joke about cats. - Took: 3.7253699579996464 seconds\n",
      "*************************\n",
      "*  Tell me a joke about cats. - Output: Why don't cats play poker in the jungle? Too many cheetahs.\n",
      "*************************\n",
      "17:04:17 What is Relativity theory - CANCELLED after 5.0010516250004 seconds\n",
      "17:04:17 Give me a fun fact about space. - CANCELLED after 5.000645500000246 seconds\n",
      "17:04:17 What is Gravity - CANCELLED after 4.996879040998465 seconds\n",
      "17:04:17 Timeout occurred! Not all LLM calls completed within 5 seconds\n",
      "None\n",
      "Total time taken for all LLM calls: 5.003857499999867 seconds\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import time, datetime\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def now():\n",
    "    return datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "\n",
    "code_start = time.perf_counter()\n",
    "\n",
    "async def call_llm_async(prompt: str):\n",
    "    start_time = time.perf_counter()\n",
    "    print(f\"{now()} Calling LLM with prompt: {prompt}\")\n",
    "    try:\n",
    "        response = await asyncio.to_thread(client.responses.create,\n",
    "            model=\"gpt-5-nano\",\n",
    "            input=prompt\n",
    "        )\n",
    "        end_time = time.perf_counter()\n",
    "        total_time = end_time - start_time\n",
    "        print(f\"{now()} {prompt} - Took: {total_time} seconds\")\n",
    "        print(f\"{'*'*25}\\n*  {prompt} - Output: {response.output_text}\\n{'*'*25}\")\n",
    "        return {\"response\": response.output_text, \"time_taken\": total_time} \n",
    "\n",
    "    except asyncio.CancelledError:\n",
    "        # This happens when wait_for times out and cancels the task\n",
    "        end_time = time.perf_counter()\n",
    "        total_time = end_time - start_time\n",
    "        print(f\"{now()} {prompt} - CANCELLED after {total_time} seconds\")\n",
    "        return {\"response\": None, \"time_taken\": total_time, \"status\": \"cancelled\"}\n",
    "\n",
    "try:\n",
    "    result = await asyncio.wait_for(\n",
    "        asyncio.gather(\n",
    "        call_llm_async(\"What is Relativity theory\"),\n",
    "        call_llm_async(\"Tell me a joke about cats.\"),\n",
    "        call_llm_async(\"What's the capital of India?\"),\n",
    "        call_llm_async(\"Give me a fun fact about space.\"),\n",
    "        call_llm_async(\"What's the capital of France?\"),\n",
    "        call_llm_async(\"What is Gravity\")),\n",
    "        timeout=5\n",
    "    )\n",
    "except asyncio.TimeoutError as e:\n",
    "    print(f\"{now()} Timeout occurred! Not all LLM calls completed within 5 seconds\")\n",
    "    result = None\n",
    "    \n",
    "code_end = time.perf_counter()\n",
    "total_time = code_end - code_start\n",
    "\n",
    "print(result)\n",
    "print(f\"Total time taken for all LLM calls: {total_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b932e0",
   "metadata": {},
   "source": [
    "## Async OpenAI Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c1f4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Answer: Async marks a function as asynchronous and makes it return a promise, while await pauses execution inside that function until the awaited promise resolves and yields its value.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from openai import AsyncOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "load_dotenv()\n",
    "\n",
    "client = AsyncOpenAI()\n",
    "\n",
    "async def llm_call(prompt: str) -> str:\n",
    "    response = await client.responses.create(\n",
    "        model=\"gpt-5-nano\",\n",
    "        input=prompt\n",
    "    )\n",
    "    return response.output_text\n",
    "\n",
    "async def main():\n",
    "    result = await llm_call(\"Explain async and await in one sentence\")\n",
    "    print(\"ðŸ¤– Answer:\", result)\n",
    "\n",
    "# Run the event loop\n",
    "asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-ai (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
